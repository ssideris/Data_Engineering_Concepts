{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771023a6",
   "metadata": {},
   "source": [
    "## Uploading Data on [Google Cloud Storage](https://cloud.google.com/storage/docs/introduction) and Querying them Using [BigQuery](https://cloud.google.com/bigquery/docs/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0194d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex;\">              <img src=\"Images/author_pic.jpg\" alt=\"author profile pic\" style=\"width:8%;                      border-radius:100%; border: 1px solid black;\"/>              <div style=\"float: right; margin-left:3%\">              <p style=\" font-size: 130%; margin-top:10%; \">By Stamatis Sideris</p>              <p style=\"font-size: 100%;\">Updated as of: September 16, 2023</p>              </div>              </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"display: flex;\"> \\\n",
    "             <img src=\"Images/author_pic.jpg\" alt=\"author profile pic\" style=\"width:8%; \\\n",
    "                     border-radius:100%; border: 1px solid black;\"/> \\\n",
    "             <div style=\"float: right; margin-left:3%\"> \\\n",
    "             <p style=\" font-size: 130%; margin-top:10%; \">By Stamatis Sideris</p> \\\n",
    "             <p style=\"font-size: 100%;\">Updated as of: September 16, 2023</p> \\\n",
    "             </div> \\\n",
    "             </div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3569b92",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3825d",
   "metadata": {},
   "source": [
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Prerequisites](#prerequisites)\n",
    "\n",
    "[3. Creating a GCS Bucket](#creating-a-gcs-bucket)\n",
    "\n",
    "[4. Uploading Data on GCS using Flow-as-a-Code](#uploading-data-on-gcs-using-flow-as-a-code)\n",
    "\n",
    "[5. Querying GCS using BigQuery - Partitioning and Clustering](#querying-gcs-using-bigquery-partitioning-and-clustering)\n",
    "\n",
    "[6. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9baea52",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the ever-evolving landscape of cloud computing, [GCP](https://cloud.google.com/gcp?utm_source=google&utm_medium=cpc&utm_campaign=emea-gr-all-en-bkws-all-all-trial-b-gcp-1011340&utm_content=text-ad-none-any-DEV_c-CRE_669760174267-ADGP_Hybrid+%7C+BKWS+-+BRO+%7C+Txt+~+GCP+~+General%23v1-KWID_43700077708210624-aud-1642982741513:kwd-4406040420-userloc_9061578&utm_term=KW_google+cloud-NET_g-PLAC_&&gad=1&gclid=CjwKCAjwpJWoBhA8EiwAHZFzfvcjebApmQTkoiqwbu-mDPQXYGmVYRfLpPJC_yPCgpzx8yXbVCfvpBoCEGIQAvD_BwE&gclsrc=aw.ds&hl=en) stands as a powerhouse, offering a plethora of tools and services designed to empower businesses with the ability to store, manage, and analyze data at scale. [Google Cloud Storage](https://cloud.google.com/storage/docs/introduction) and [BigQuery](https://cloud.google.com/bigquery/docs/introduction) are two cornerstones of Google's cloud ecosystem, renowned for their reliability, scalability, and ease of use.\n",
    "\n",
    "In this tutorial, we will embark on a journey to demystify the process of uploading data to Google Cloud Storage, a versatile and cost-effective object storage service that allows you to securely store your data in the cloud. We will explore various methods of data ingestion, ensuring that your data is readily available for analysis.\n",
    "\n",
    "Once your data is securely stored in Google Cloud Storage, we will dive into the world of BigQuery, Google's fully-managed, serverless, and highly scalable data warehouse. We'll walk you through the process of creating datasets, tables, and how to query your data with SQL-like syntax. Whether you're working with structured or semi-structured data, BigQuery's capabilities are designed to handle it all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b26fc",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "As always, I am going to start by using a VM instance provided by Google, as proposed [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Virtual%20Machine%20Instance%20Creation%20using%20Google%20Cloud.pdf). This is a very helpful step to deal with the low specs of your computer but remains optional.\n",
    "\n",
    "Moreover, make sure to deploy a Virtual Environment with all the needed dependencies. I use Anaconda to create one. Inside the VE, we pip install :the dependencies that are needed for this tutorial using a requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413243a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The requirements.txt\n",
    "pandas==2.0.3\n",
    "protobuf==4.23.4\n",
    "pandas-gbq==0.19.2\n",
    "psycopg2-binary==2.9.6\n",
    "opendatasets==0.1.22\n",
    "python-dotenv==1.0.0\n",
    "google-cloud-storage==2.10.0\n",
    "pyarrow==13.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n ecommerce python=3.11\n",
    "conda activate ecommerce\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8cb76a",
   "metadata": {},
   "source": [
    "### Creating a GCS Bucket\n",
    "\n",
    "GCS Buckets serve as containers for storing and organizing your data objects, which can be files, images, videos, backups, or any other type of unstructured data. We proceed by creating one.\n",
    "\n",
    "First, visit the [Google Cloud page](https://cloud.google.com/gcp?utm_source=google&utm_medium=cpc&utm_campaign=emea-gr-all-en-bkws-all-all-trial-e-gcp-1011340&utm_content=text-ad-none-any-DEV_c-CRE_501834844520-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt+~+GCP+~+General%23v1-KWID_43700061559683578-aud-606988877934:kwd-6458750523-userloc_9061578&utm_term=KW_google+cloud-NET_g-PLAC_&&gad=1&gclid=CjwKCAjwpJWoBhA8EiwAHZFzfnlfZOwyXCG_0iSqrLmO6oCTIUPrbM4dl9NhIrD622715Jt2KXFfaBoC1LcQAvD_BwE&gclsrc=aw.ds&hl=en) and sign in. Then, visit [Console](https://console.cloud.google.com/welcome?_ga=2.209774812.-898003105.1673432341&_gac=1.222037610.1694886521.CjwKCAjwpJWoBhA8EiwAHZFzfnlfZOwyXCG_0iSqrLmO6oCTIUPrbM4dl9NhIrD622715Jt2KXFfaBoC1LcQAvD_BwE&hl=en&authuser=2&project=e-commerce-shipping-392016) to enter the Google Cloud's developer mode and create a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a4213",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d862f4eb",
   "metadata": {},
   "source": [
    "We proceed on visiting the Google Storage tab on the upper left taskbar. There we choose to create a new Bucket. We set the unique name of the Bucket (mine is ecommerce_bucket_tutorial) and keep the rest information on default settings. After the creation, the bucket should be displayed on the tab's start-up page as an object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea69db4",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture2.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9c6fb",
   "metadata": {},
   "source": [
    "Then, we visit the IAM & ADMIN tab from the upper left panel, and choose the Service Accounts tab. There we will create a service account that will enable us accessing the GCS Bucket we just created. To do so, click on \"CREATE SERVICE ACCOUNT\", give a name to the account and set the role to Cloud Storage Admin. The new service account should be displayed on the tab's starting page together with any other service accounts you might have created in the past:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a335c92",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture3.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69db656",
   "metadata": {},
   "source": [
    "Finally, we visit the created service account and go to the tab \"KEYS\". There we choose to create a new key in order to access the service account remotely. The key should be downloaded in json format on your local downloads folder and moved to your local project's directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7698e",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture4.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c2be8",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture5.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240d77c",
   "metadata": {},
   "source": [
    "### Uploading Data on GCS using Flow-as-a-Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3041fa0",
   "metadata": {},
   "source": [
    "We use dotenv library in order to install environment variables. The .env file to use is including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = \"2019-Dec,2019-Nov,2019-Oct,2020-Jan,2020-Feb\"\n",
    "data_url = \"https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop/download?datasetVersionNumber=6\"\n",
    "raw_path = \"ecommerce-events-history-in-cosmetics-shop/\"\n",
    "parquet_path = \"parquet_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa6434",
   "metadata": {},
   "source": [
    "We are going to use the following code in Python in order to download data from Kaggle, transform them, save them locally as parquet files in a directory called \"parquet_files\" and finally upload them to GCS in a directory called \"parquet_files\". The library google.cloud is helping us to connect to the GCS Bucket using the key.json we downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000dbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import opendatasets as od\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import storage\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "def extract_data(path: str) :\n",
    "    # read each csv in chunks of 100000 rows \n",
    "    df_iter = pd.read_csv(path, iterator=True, chunksize=100000)\n",
    "    df = next(df_iter)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_data(i, df, parquet_path):\n",
    "    # Set the datetime format, create new column date and drop a column that is full of Nulls\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "    df['date'] = df['event_time'].dt.date\n",
    "    df = df.drop('category_code', axis=1)\n",
    "\n",
    "    # Save the DataFrame as a Parquet file\n",
    "    pq.write_table(pa.Table.from_pandas(df), f\"{parquet_path}{i}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def upload_to_gcs(i, parquet_path):\n",
    "    # Initialize Google Cloud Storage client\n",
    "    client = storage.Client.from_service_account_json('e-commerce-shipping-392016-0aa683b08b01.json')  # Replace with your JSON key file\n",
    "    \n",
    "    # Define your GCS bucket and object names\n",
    "    bucket_name = 'ecommerce_bucket_tutorial'\n",
    "    object_name = f'{parquet_path}{i}'  # Replace with your desired GCS object path\n",
    "    \n",
    "    # Upload the Parquet file to GCS\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(object_name)\n",
    "    blob.upload_from_filename(f'{parquet_path}{i}')\n",
    "\n",
    "    print(f'Parquet file {parquet_path} uploaded to gs://{bucket_name}/{object_name}')\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    od.download(os.getenv(\"data_url\"))    \n",
    "\n",
    "def main_flow(csv_name:str=\"2019-Dec.csv,2019-Nov.csv,2019-Oct.csv,2020-Jan.csv,2020-Feb.csv\", raw_path:str=\"ecommerce-events-history-in-cosmetics-shop/\", data_url:str=\"https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop/download?datasetVersionNumber=6\", parquet_path:str=\"parquet_files/\"):\n",
    "    \n",
    "    # Download dataset from Kaggle\n",
    "    download_data()\n",
    "    for i in csv_name.split(','):\n",
    "        path = raw_path + i + \".csv\"\n",
    "        raw_data = extract_data(path)\n",
    "        data = transform_data(i, raw_data, parquet_path)  # Pass the Parquet file path\n",
    "        upload_to_gcs(i, parquet_path)  # Pass the Parquet file path\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_dotenv()\n",
    "    csv_name = os.getenv(\"csv_name\")\n",
    "    raw_path = os.getenv(\"raw_path\")\n",
    "    data_url = os.getenv(\"data_url\")\n",
    "    parquet_path = os.getenv(\"parquet_path\")\n",
    "    main_flow(csv_name, raw_path, data_url, parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a205cb5",
   "metadata": {},
   "source": [
    "After running the script, the files should be uploaded on the GCS Bucket we created before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a88a60",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture6.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84a0f7",
   "metadata": {},
   "source": [
    "### Querying GCS using BigQuery - Partitioning and Clustering\n",
    "\n",
    "BigQuery is a fully-managed, serverless, and highly scalable data warehouse and analytics platform offered by Google Cloud. It is designed for running fast and SQL-like queries on very large datasets, making it ideal for data analysis, reporting, and business intelligence. Let's start by connecting BigQuery to our GCS Bucket and query some data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f194df4",
   "metadata": {},
   "source": [
    "Firstly, visit the BigQuery tab from the upper left panel. On the left, a panel should open which includes the name of our project. Open it and from the 3 dots choose to create a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c1025",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture7.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6314d",
   "metadata": {},
   "source": [
    "There we choose GCS as our preferred source and we set the file name/names we want to include, the file format of the files, the name of the dataset we just created and the name of the table we are about to create. Here, i am selecting all the files inside the parquet_files directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f3ae4",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture8.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fdc49",
   "metadata": {},
   "source": [
    "The table should be loaded appending all the parquet files into a single table. We open the created Table and check the schema that BigQuery is using to read the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99eacbb",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture9.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e941dd",
   "metadata": {},
   "source": [
    "We proceed by composing a new query using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT event_time, product_id, category_id, brand, price \n",
    "FROM `e-commerce-shipping-392016.ecommerce_data.2019-Dec` \n",
    "LIMIT 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5826b30",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture10.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41c357",
   "metadata": {},
   "source": [
    "Moreover, we perform an aggregated query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT product_id, AVG(price) AS Average_Price FROM `e-commerce-shipping-392016.ecommerce_data.ecommerce_data_all` \n",
    "WHERE date = '2019-11-01'\n",
    "GROUP BY product_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b08ca",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture11.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebcc035",
   "metadata": {},
   "source": [
    "At this point, it is important to talk about BigQuery's advantages. BigQuery is enabling us to Partition and Cluster our tables. BigQuery Partitioning and Clustering are two techniques used to optimize query performance and reduce costs when working with large datasets.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15a50f",
   "metadata": {},
   "source": [
    "Partitioning involves dividing a large table into smaller, more manageable pieces based on a specific column or field. Each partition acts as a separate unit, and queries only need to process the relevant partitions, which can significantly improve query performance. You may partition a table by:\n",
    "\n",
    "- Time-unit column: tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table.\n",
    "- Ingestion time: tables are partitioned based on the timestamp when BigQuery ingests the data.\n",
    "- Integer range: tables are partitioned based on an integer column.\n",
    "\n",
    "![image.png](Images/Picture15.png)\n",
    "\n",
    "We will use the column \"date\" we generated in python, which is of type DATE, to create a new partitioned table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE ecommerce_data.ecommerce_data_all_partitioned\n",
    "PARTITION BY date AS\n",
    "SELECT * FROM e-commerce-shipping-392016.ecommerce_data.ecommerce_data_all; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c6093",
   "metadata": {},
   "source": [
    "We perform again the aggregated query on the partitioned table and we observe that the amount of data processed by the query is now 2.29MB in contrast to the 14.11MB needed by the previous query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a74d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT product_id, AVG(price) AS Average_Price\n",
    "FROM `e-commerce-shipping-392016.ecommerce_data.ecommerce_data_all_partitioned`\n",
    "WHERE date = '2019-11-01'\n",
    "GROUP BY product_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426dd43",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f01ce1",
   "metadata": {},
   "source": [
    "On the other hand, Clustering involves organizing the data within each partition based on the values in one or more columns. This improves query performance by physically grouping related data together, making it more efficient to retrieve. \n",
    "\n",
    "![image.png](Images/Picture14.png)\n",
    "\n",
    "We create a new table that is partitioned by date and clustered by product_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45437c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE `e-commerce-shipping-392016.ecommerce_data.ecommerce_data_all_partitioned_clustered`\n",
    "PARTITION BY date\n",
    "CLUSTER BY product_id AS\n",
    "SELECT * FROM e-commerce-shipping-392016.ecommerce_data.ecommerce_data_all;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420664e",
   "metadata": {},
   "source": [
    "We observe again, that the amount of data that is processed is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0524f7",
   "metadata": {},
   "source": [
    "![image.png](Images/Picture13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ff0a7",
   "metadata": {},
   "source": [
    "By effectively using Partitioning and Clustering we can increase the efficiency of our queries, making them faster and cost-effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411482b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Throughout this tutorial, we've demonstrated how to set up GCS and BigQuery, how to seamlessly transfer data between them, and how to perform powerful analytics and querying tasks with BigQuery. We've also covered best practices and tips to optimize your workflow, ensuring efficient data management and analysis. In future tutorials, we will explore BigQuery more, taking advantage of its Analytics and ML Pipeline abilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
