{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84093c45",
   "metadata": {},
   "source": [
    "## Flow-as-a-Code [Deployment](https://docs.prefect.io/2.11.3/concepts/deployments/) using Prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cbf298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex;\">              <img src=\"Images/author_pic.jpg\" alt=\"author profile pic\" style=\"width:8%;                      border-radius:100%; border: 1px solid black;\"/>              <div style=\"float: right; margin-left:3%\">              <p style=\" font-size: 130%; margin-top:10%; \">By Stamatis Sideris</p>              <p style=\"font-size: 100%;\">Updated as of: August 6, 2023</p>              </div>              </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"display: flex;\"> \\\n",
    "             <img src=\"Images/author_pic.jpg\" alt=\"author profile pic\" style=\"width:8%; \\\n",
    "                     border-radius:100%; border: 1px solid black;\"/> \\\n",
    "             <div style=\"float: right; margin-left:3%\"> \\\n",
    "             <p style=\" font-size: 130%; margin-top:10%; \">By Stamatis Sideris</p> \\\n",
    "             <p style=\"font-size: 100%;\">Updated as of: August 6, 2023</p> \\\n",
    "             </div> \\\n",
    "             </div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768a98e",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Prerequisites](#prerequisites)\n",
    "\n",
    "[3. Creating a Flow-as-a-Code using Python and Prefect](#creating-an-etl-flow-using-python)\n",
    "\n",
    "[4. Deploying a Local Flow using CLI and Prefect UI](#deploying-a-local-flow-using-cli-and-prefect-ui)\n",
    "\n",
    "[5. Parameterizing the Flow](#parameterizing-the-flow)\n",
    "\n",
    "[6. Containerizing the Deployment for Running using Docker](#containerizing-the-deployment-for-running-using-docker)\n",
    "\n",
    "[7. Git Cloning the Deployment for Version Controlling](#git-cloning-the-deployment-for-version-controlling)\n",
    "\n",
    "[8. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b086696",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "A deployment is a server-side concept that encapsulates a flow, allowing it to be scheduled and triggered via API. The deployment stores metadata about where your flow's code is stored and how your flow should be run.\n",
    "\n",
    "At a high level, creating a deployment for a Prefect workflow means packaging workflow code, settings, and infrastructure configuration so that the workflow can be managed via the Prefect API and run remotely by a Prefect agent.\n",
    "\n",
    "In this tutorial, we will deploy a Prefect workflow and manage it using the CLI and the Prefect UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d800f04",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "As always, I am going to start by using a VM instance provided by Google, as proposed [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Virtual%20Machine%20Instance%20Creation%20using%20Google%20Cloud.pdf). This is a very helpful step to deal with the low specs of your computer but also, it is optional.\n",
    "\n",
    "Moreover, make sure to deploy a Virtual Environment with all the needed dependencies. I use anaconda for creating one. Inside the VE, we pip install Prefect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fdc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n ecommerce python=3.11\n",
    "conda activate ecommerce\n",
    "pip install prefect==2.10.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c264d9",
   "metadata": {},
   "source": [
    "### Creating a Flow-as-a-Code using Python and Prefect\n",
    "\n",
    "The creation of the Flow-as-a-Code has been described [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Monitoring%20Flow-as-a-Code%20using%20Prefect). Prefect enables us to use Python functions to create packages of tasks, in this case ETL procedures, and easily monitor and orchestrate them using decorators. \n",
    "\n",
    "We will add a flow decorator over the main_flow function as it is the one that shows the sequence in which the tasks should be executed and then we will add a task decorator over each other function that is working as a step of the flow. Each decorator can be parameterized, setting for example a name for each task, enabling log_prints and the number of retries before failure of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import timedelta\n",
    "import opendatasets as od\n",
    "import os\n",
    "# import prefect\n",
    "from prefect import task, flow\n",
    "# import dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "@task(name=\"Read Data as csv\", log_prints=True, retries=3)\n",
    "def extract_data(path: str) :\n",
    "    # read each csv in chunks of 100000 rows \n",
    "    df_iter = pd.read_csv(path, iterator=True, chunksize=100000)\n",
    "    df = next(df_iter)\n",
    "\n",
    "    return df\n",
    "\n",
    "@task(name=\"Transform data\", log_prints=True, retries=3)\n",
    "def transform_data(df) :\n",
    "    # set the datetime format and drop a table that is full of Nulls\n",
    "    df['event_time'] = pd.to_datetime(df['event_time']) \n",
    "    df = df.drop('category_code', axis=1)\n",
    "    return df\n",
    "\n",
    "@task(name=\"Load data to PostgreSQL\", log_prints=True, retries=3)\n",
    "def load_data(table_name, df):\n",
    "    # create an engine that connects to the postgres database\n",
    "    engine = create_engine(f'postgresql://{os.getenv(\"user\")}:{os.getenv(\"password\")}@{os.getenv(\"host\")}:{os.getenv(\"port\")}/{os.getenv(\"db\")}')\n",
    "    # append each chunk to the table - the chunk method is needed because .to_sql function cannot handle large volume of data\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "    print(\"Finished ingesting data into the postgres database\")\n",
    "\n",
    "@task(name=\"Download data from Kaggle\", log_prints=True, retries=3)\n",
    "def download_data():\n",
    "    od.download(os.getenv(\"data_url\"))\n",
    "\n",
    "@task(name=\"Load Environment Variables\", log_prints=True, retries=3)\n",
    "def load_env():\n",
    "    load_dotenv()\n",
    "\n",
    "def log_subflow(table_name: str):\n",
    "    print(f\"Logging Subflow for: {table_name}\")\n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow():\n",
    "    # load environment variables\n",
    "    load_env()\n",
    "    # download dataset from Kaggle\n",
    "    download_data()\n",
    "    log_subflow(os.getenv(\"table_name\"))\n",
    "    for i in os.getenv(\"csv_name\").split(','):\n",
    "        path = os.getenv(\"data_path\") + i\n",
    "        raw_data = extract_data(path)\n",
    "        data = transform_data(raw_data)\n",
    "        #load_data(os.getenv(\"table_name\"), data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cbaef",
   "metadata": {},
   "source": [
    "Finally, we create a \"flows/flow_1\" directory and move the python script there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ff5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir flows/flow_1\n",
    "mv ./etl.py ./flows/flow_1/etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccc996",
   "metadata": {},
   "source": [
    "### Deploying a Local Flow using CLI and Prefect UI\n",
    "\n",
    "To start the deployment of a workflow, we visit the directory where the flow is stored, and use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cebdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ee7d6",
   "metadata": {},
   "source": [
    "The command will start an interactive prompt which will ask us for our preferred deployment. This time we will choose the local deployment, which is the simplest one and derives the flow from our local file storage system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0dd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b65e4b",
   "metadata": {},
   "source": [
    "Three files are created inside the flows directory. The .prefectignore is for avoiding pushing sensitive prefect data to the git repo. The .prefect directory works as a directory for mounting prefect's metadata. Finally, the most important file, prefect.yaml is a configuration file for our flow. \n",
    "\n",
    "Let's take a look inside the yaml file. First of all, there are no build and push sections. This is happening as we do not use docker to containerize our flow and also we use the local file stoarege system so there is no need for pushing/uploading the code to remote locations. Moreover, the pull section is responsible for telling prefect where to locate our flow. Finally, the deployments section is responsible for configuring the flow, for example scheduling or parameterizing the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_1\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build: null\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push: null\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.set_working_directory:\n",
    "    directory: /home/sgsid/flows/flow_1\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name: null\n",
    "  version: null\n",
    "  tags: []\n",
    "  description: null\n",
    "  schedule: {}\n",
    "  flow_name: null\n",
    "  entrypoint: null\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: null\n",
    "    work_queue_name: null\n",
    "    job_variables: {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e72caf",
   "metadata": {},
   "source": [
    "Prefect is offering an easy way of configuring the flows using an interactive prompt. In order to make our flow visible to the UI we must first deploy it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3df4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f241f0",
   "metadata": {},
   "source": [
    "The command will start an interactive prompt which will ask us questions on configuring the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4132ad0",
   "metadata": {},
   "source": [
    "Firstly, we choose the flow we want to deploy and set the name to \"etl-deployment-1\". \n",
    "Then, we schedule it using a [cron job](https://www.hostinger.com/tutorials/cron-job). For a free cron expression convertor, you could refer [here](https://crontab.guru). In order to set the prefect agent to run the flow every day at 2am, we will use the following cron expression:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 2 * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8ac94",
   "metadata": {},
   "source": [
    "Moreover, we set the entrypoint to the path to the .py file containing the flow we want to deploy (relative to the root directory of your development folder) combined with the name of the flow function. Mine is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b21282",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/sgsid/flows/flow_1/etl.py:etl_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b0f8b",
   "metadata": {},
   "source": [
    "Finally, we create a [work pool and a worker](https://docs.prefect.io/2.11.3/concepts/work-pools/) responsible for pulling and start running our deployment based on our scheduling. We name the work pool 'etl' and we choose type 'process' for our worker. This type of worker is responsible for simply pulling the flow process from the local system and running the flow process when the scheduled time has come. The interactive prompt will ask us to create the work pool automatically if no other work-pools exist but if we wanted to create a work pool before the deployment, we could use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect work-pool create --type docker docker-etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62c46d",
   "metadata": {},
   "source": [
    "and use this command to see the created and available work pools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect work-pool ls "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d08b5",
   "metadata": {},
   "source": [
    "By now, the deployment should be ready and will be added automatically inside the prefect.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e154821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_1\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.set_working_directory:\n",
    "    directory: /home/sgsid/flows/flow_1\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name:\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  schedule: {}\n",
    "  flow_name:\n",
    "  entrypoint:\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name:\n",
    "    work_queue_name:\n",
    "    job_variables: {}\n",
    "- name: etl-deployment-1\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  entrypoint: etl.py:etl_flow\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: etl-local\n",
    "    work_queue_name:\n",
    "    job_variables: {}\n",
    "  schedule:\n",
    "    cron: 0 2 * * *\n",
    "    timezone: UTC\n",
    "    day_or: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913d397",
   "metadata": {},
   "source": [
    "To start the deployment using the configuration file prefect.yaml, use the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da797e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect deploy -n etl-deployment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bbfe1f",
   "metadata": {},
   "source": [
    "Let's take a look on our deployment using the Prefect's UI. To do so, start a prefect server on a different terminal using this command and visit the provided url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect server start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45098b4c",
   "metadata": {},
   "source": [
    "Visiting the \"Flows\" tab, we can see the flow \"Main Flow\" as described in the etl.py. The flow says that has been deployed. In order to see the Deployment, we visit the Deployments tab, where we can see that our newly established deployment is stored there including our Main Flow as described in tab \"Flows\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0583f7",
   "metadata": {},
   "source": [
    "Three Flow runs have been scheduled to run at 2am for the next three days. Each running is associated with the work pool \"etl\" we created before. If we visit the \"Work Pools\" tab, we can see the \"etl-local\" work pool stored there, including three scheduled works and a worker to execute them on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4469f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45cda4",
   "metadata": {},
   "source": [
    "After finishing with the deployment, to execute flow runs from this deployment, start a worker in a separate terminal that pulls work from the 'etl' work pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect worker start --pool 'etl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2365d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29982b33",
   "metadata": {},
   "source": [
    "The worker will start running the scheduled workflows when their time has arrived. Then to start running the deployment immediately, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2903e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect deployment run 'Main Flow/etl-deployment-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125ba41",
   "metadata": {},
   "source": [
    "As a result, a workflow is deployed inside the \"etl-local\" work-pool and the responsible worker will start running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48daddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d9b26",
   "metadata": {},
   "source": [
    "### Parameterizing the Flow\n",
    "\n",
    "By now, we are using a .env file and the library dotenv in order to parameterize our flow. This is a great method when parameters include sensitive data we want to avoid making public, such as passwords. The problem is that prefect is unable to locate the rest parameters that characterize each flow and so present them together with the rest info the API generates for each flow.\n",
    "\n",
    "Prefect is offering an easy way of parameterizing the code and then managing the parameters in each run. To do so, we should perform some changes to our initial etl.py. In the following updated etl.py, we add our parameters inside the \"Main Flow\" function and set some default values. This way our deployment is able to recognise the parameters and their default values. Then, we use the \"if __name__ == '__main__':\" method to declare there the sensitive parameters using the dotenv library and call the main flow. This way, when the deployment is called it will run using the .env parameters for the sensitive parameters and the default parameters for all the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import timedelta\n",
    "import opendatasets as od\n",
    "import os\n",
    "# import prefect\n",
    "from prefect import task, flow\n",
    "# import dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "@task(name=\"Read Data as csv\", log_prints=True, retries=3)\n",
    "def extract_data(path: str) :\n",
    "    # read each csv in chunks of 100000 rows \n",
    "    df_iter = pd.read_csv(path, iterator=True, chunksize=100000)\n",
    "    df = next(df_iter)\n",
    "\n",
    "    return df\n",
    "\n",
    "@task(name=\"Transform data\", log_prints=True, retries=3)\n",
    "def transform_data(df) :\n",
    "    # set the datetime format and drop a table that is full of Nulls\n",
    "    df['event_time'] = pd.to_datetime(df['event_time']) \n",
    "    df = df.drop('category_code', axis=1)\n",
    "    return df\n",
    "\n",
    "@task(name=\"Load data to PostgreSQL\", log_prints=True, retries=3)\n",
    "def load_data(table_name, df, user, password, host, port, db):\n",
    "    # create an engine that connects to the postgres database\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "    # append each chunk to the table - the chunk method is needed because .to_sql function cannot handle large volume of data\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "    print(\"Finished ingesting data into the postgres database\")\n",
    "\n",
    "@task(name=\"Download data from Kaggle\", log_prints=True, retries=3)\n",
    "def download_data():\n",
    "    od.download(os.getenv(\"data_url\"))    \n",
    "\n",
    "def log_subflow(table_name: str):\n",
    "    print(f\"Logging Subflow for: {table_name}\")\n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(table_name:str=\"ecommerce_data_all\", csv_name:str=\"2019-Dec.csv,2019-Nov.csv,2019-Oct.csv,2020-Jan.csv,2020-Feb.csv\", data_path:str=\"ecommerce-events-history-in-cosmetics-shop/\", data_url:str=\"https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop/download?datasetVersionNumber=6\", user:str=\"user\", password:str=\"password\", host:str=\"host\", port:str=\"port\", db:str=\"db\"):\n",
    "    # download dataset from Kaggle\n",
    "    print(csv_name)\n",
    "    download_data()\n",
    "    for i in csv_name.split(','):\n",
    "        path = data_path + i\n",
    "        raw_data = extract_data(path)\n",
    "        data = transform_data(raw_data)\n",
    "        #load_data(table_name, data, , user, password, host, port, db)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # load environment variables\n",
    "    load_dotenv()\n",
    "\n",
    "    # declare the variables\n",
    "    user=os.getenv(\"user\")\n",
    "    password=os.getenv(\"password\")\n",
    "    host=os.getenv(\"host\")\n",
    "    port=os.getenv(\"port\")\n",
    "    db=os.getenv(\"db\")\n",
    "\n",
    "    # start workflow\n",
    "    log_subflow(table_name)\n",
    "    main_flow(table_name, csv_name, data_path, data_url, user, password, host, port, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8f104",
   "metadata": {},
   "source": [
    "In order to set the values of the rest parameters, we will open the prefect.yaml file which was automatically created when we configured our deployment. There, we visit the key \"parameters\" and add there the names of the parameters and their values you want to run the deployment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_1\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.set_working_directory:\n",
    "    directory: /home/sgsid/ecommerce/flows/flow_1\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name:\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  schedule: {}\n",
    "  flow_name:\n",
    "  entrypoint:\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name:\n",
    "    work_queue_name:\n",
    "    job_variables: {}\n",
    "- name: etl-deployment-1\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  entrypoint: etl.py:main_flow\n",
    "  parameters: {\"table_name\":\"ecommerce_data_all\", \"csv_name\":\"2019-Dec.csv,2019-Nov.csv\", \"data_path\":\"ecommerce-events-history-in-cosmetics-shop/\", \"data_url\":\"https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop/download?datasetVersionNumber=6\"}\n",
    "  work_pool:\n",
    "    name: etl\n",
    "    work_queue_name:\n",
    "    job_variables: {}\n",
    "  schedule:\n",
    "    cron: 0 2 * * *\n",
    "    timezone: UTC\n",
    "    day_or: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6aa78",
   "metadata": {},
   "source": [
    "Next time we will start this deployment, the parameters \"table_name\", \"csv_name\", \"data_path\" and \"data_url\" should change based on the values in the prefect.yaml while the rest sensitive parameters \"user\", \"password\", \"host\",\"port\" and \"db\" will take values based on the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1f75e",
   "metadata": {},
   "source": [
    "### Containerizing the Deployment for Running using Docker\n",
    "\n",
    "The containerization of a workflow has been described analytically [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Monitoring%20Flow-as-a-Code%20using%20Prefect). \n",
    "\n",
    "Let's now see how to containerize a deployment and create a network so that we can upload our transformed data to a postgres db container. To do so, we need to create a completely new deployment configured by a new prefect.yaml in the flow_2 directory.\n",
    "\n",
    "To start, let's create a worker that will be responsible to pull a docker image and run our deployment. To do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect work-pool create --type docker docker-etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61d84d",
   "metadata": {},
   "source": [
    "Next, create a custom made Dockerfile that will include our flow and the requirements needed for it to run. The Dockerfile will be used to create the image of our deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ff0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM prefecthq/prefect:2.10.21-python3.11 \n",
    "\n",
    "ADD . /opt/prefect/flows/flow_2\n",
    "\n",
    "RUN pip install -r /opt/prefect/flows/flow_2/requirements.txt --trusted-host pypi.python.org --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80506eeb",
   "metadata": {},
   "source": [
    "Then, use the following command to initialize a deployment that will run inside a Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect init --recipe docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d524251",
   "metadata": {},
   "source": [
    "Prefect will ask you to set a name for your image and a tag. Then, it will proceed to create a prefect.yaml configuration file. The file is formated so that it will help as create a deployment that is able to run inside a docker container. The provided file is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a32add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: test\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "- prefect_docker.deployments.steps.build_docker_image:\n",
    "    id: build_image\n",
    "    requires: prefect-docker>=0.3.1\n",
    "    image_name: test\n",
    "    tag: latest\n",
    "    dockerfile: auto\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "- prefect_docker.deployments.steps.push_docker_image:\n",
    "    requires: prefect-docker>=0.3.1\n",
    "    image_name: '{{ build_image.image_name }}'\n",
    "    tag: '{{ build_image.tag }}'\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.set_working_directory:\n",
    "    directory: /opt/prefect/test\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name: null\n",
    "  version: null\n",
    "  tags: []\n",
    "  description: null\n",
    "  schedule: {}\n",
    "  flow_name: null\n",
    "  entrypoint: null\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: null\n",
    "    work_queue_name: null\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120d973",
   "metadata": {},
   "source": [
    "To start the deployment using the yaml file, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139aa7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db10331",
   "metadata": {},
   "source": [
    "An interactive prompt will guide you with the configuration of the deployment asking you for the name of the deployment, what flow.py to be used as entrypoint, if you want to schedule the deployment and some configuration questions about the Docker image we will discuss next. After all, Prefect will start building the image.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313147e",
   "metadata": {},
   "source": [
    "Before starting the deployment, some more steps could be taken:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dfa80",
   "metadata": {},
   "source": [
    "Prefect is giving the ability to push the created image to more places than our local repository. We will use Docker Hub's online repository to store the created image and make it accessible to eveyrone with authorization rights. \n",
    "\n",
    "Docker Hub is a cloud based platform offered by Docker where you can store privately or share publicly your images or pull other public images. We can easily create a repo by signing up and visiting the Repositories Tab. There we set a name for our repo and moderate its visibility settings. A free account is allowed to have only one repo. In my case, i created a repo called \"etl\" under the username \"stamatissideris\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5067b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba199538",
   "metadata": {},
   "source": [
    "After creating the repo, we are ready to use another Prefect's functionality, the [Blocks](https://docs.prefect.io/2.11.3/concepts/blocks/). Blocks are a primitive within Prefect that enables the storage of configuration and provides an interface for interacting with external systems. With blocks, you can securely store credentials for authenticating with services like AWS, GitHub, Slack, and any other system you'd like to orchestrate with Prefect.\n",
    "\n",
    "In our case, we will use a Block to automate the connection of our deployment to the Docker Hub Repository we just created. To do so, we visit the UI and the tab Blocks. Then, we choose to create a new \"Docker Registry Credentials\" Block. We set its name as \"deployment-etl-deployment-2-docker-etl-registry-creds\", the username and the password of our Docker Hub account and finally the url to our repo which should be of the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker.io/<username>/<image_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6221c3",
   "metadata": {},
   "source": [
    "Then, we will be provided by Prefect with a code snippet to add to our flow code. Add the import line at the beginning with all your other imports and the rest code at the start of your executable code. The code snippet, in my case, is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect_docker import DockerHost, DockerRegistryCredentials\n",
    "\n",
    "docker_host = DockerHost()\n",
    "docker_registry_credentials = DockerRegistryCredentials(\n",
    "    username=\"my_username\",\n",
    "    password=\"my_password\",\n",
    "    registry_url=\"registry.hub.docker.com\",\n",
    ")\n",
    "with docker_host.get_client() as client:\n",
    "    docker_registry_credentials.login(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2440617",
   "metadata": {},
   "source": [
    "Finally, we set the credentials in the yaml file's push property, using the Block we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "push:\n",
    "  - prefect_docker.deployments.steps.push_docker_image:\n",
    "      requires: prefect-docker>=0.3.1\n",
    "      image_name: '{{ build-image.image_name }}'\n",
    "      tag: '{{ build-image.tag }}'\n",
    "      credentials: '{{ prefect_docker.docker-registry-credentials.docker_registry_creds_name\n",
    "        }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908852d",
   "metadata": {},
   "source": [
    "As for the pull property, we set the working directory that is going to be used each time we pull our image to run it. This is the local path inside the image where the flow is stored. In my case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366acda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull:\n",
    "  - prefect.deployments.steps.set_working_directory:\n",
    "      directory: /opt/prefect/flows/flow_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e31fa1",
   "metadata": {},
   "source": [
    "The final configuration file should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_2\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "- prefect_docker.deployments.steps.build_docker_image:\n",
    "  requires: prefect-docker>=0.3.1\n",
    "  id: build-image\n",
    "  dockerfile: Dockerfile\n",
    "  image_name: docker.io/stamatissideris/etl\n",
    "  tag: latest\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "- prefect_docker.deployments.steps.push_docker_image:\n",
    "  requires: prefect-docker>=0.3.1\n",
    "  image_name: '{{ build-image.image_name }}'\n",
    "  tag: '{{ build-image.tag }}'\n",
    "  credentials: '{{ prefect_docker.docker-registry-credentials.docker_registry_creds_name}}'\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.set_working_directory:\n",
    "      directory: /opt/prefect/flows/flow_2\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name:\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  schedule: {}\n",
    "  flow_name:\n",
    "  entrypoint:\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name:\n",
    "    work_queue_name:\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'\n",
    "\n",
    "- name: etl-deployment-2\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  entrypoint: etl.py:main_flow\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: docker-etl\n",
    "    work_queue_name:\n",
    "    job_variables:\n",
    "      image: '{{ build-image.image }}'\n",
    "  schedule:\n",
    "    cron: 0 2 * * *\n",
    "    timezone: UTC\n",
    "    day_or: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d7516",
   "metadata": {},
   "source": [
    "Finally, you are ready to start building the deployment again. To do so, use the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01917090",
   "metadata": {},
   "source": [
    "Prefect will start building the docker image you specified in the configuration file, will push the image to the Docker Hub and activate the deployment. The work pool will be responsible to start running the deployment at the scheduled time and with the parameters set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd34110",
   "metadata": {},
   "source": [
    "As we can see using Docker Desktop, the image has been created locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b483f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b580d5",
   "metadata": {},
   "source": [
    "And has been pushed at the Docker Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111f910",
   "metadata": {},
   "source": [
    "Finally, we visit the Prefect UI where we can see our new development running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b34377",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1aa2d8",
   "metadata": {},
   "source": [
    "In case you do not want to configure yaml files for deployment, prefect makes it easy for you by providing you with an interactive prompt to build the deployment. Just change the values in each property to null and use the \"prefect deploy\" command. Prefect will recognise the empty yaml configuration file and will complete it by asking you questions. An example of an empty yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0162c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_2\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name:\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  schedule: {}\n",
    "  flow_name:\n",
    "  entrypoint:\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name:\n",
    "    work_queue_name:\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b972b3",
   "metadata": {},
   "source": [
    "The result after the deployment will be the same, but might follow a different structure from the one given before by including the build, push and pull properties inside the deployments property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4cee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_2\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name:\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  schedule: {}\n",
    "  flow_name:\n",
    "  entrypoint:\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name:\n",
    "    work_queue_name:\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'\n",
    "\n",
    "- name: etl-deployment-2\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  entrypoint: etl.py:main_flow\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: docker-etl\n",
    "    work_queue_name:\n",
    "    job_variables:\n",
    "      image: '{{ build-image.image }}'\n",
    "  schedule:\n",
    "    cron: 0 2 * * *\n",
    "    timezone: UTC\n",
    "    day_or: true\n",
    "  build:\n",
    "  - prefect_docker.deployments.steps.build_docker_image:\n",
    "      requires: prefect-docker>=0.3.1\n",
    "      id: build-image\n",
    "      dockerfile: Dockerfile\n",
    "      image_name: docker.io/stamatissideris/etl\n",
    "      tag: latest\n",
    "  push:\n",
    "  - prefect_docker.deployments.steps.push_docker_image:\n",
    "      requires: prefect-docker>=0.3.1\n",
    "      image_name: '{{ build-image.image_name }}'\n",
    "      tag: '{{ build-image.tag }}'\n",
    "      credentials: '{{ prefect_docker.docker-registry-credentials.docker_registry_creds_name\n",
    "        }}'\n",
    "  pull:\n",
    "  - prefect.deployments.steps.set_working_directory:\n",
    "      directory: /opt/prefect/flows/flow_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaedabd",
   "metadata": {},
   "source": [
    "### Git Cloning the Deployment for Version Controlling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefaf1eb",
   "metadata": {},
   "source": [
    "The same procedure is followed in order to git clone the flow of a deployment from a Github online repository. There are two prefect recipes that can be used in order to be provided with the appropriate yaml file and configure a git deployment. The first is using only the git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect init --recipe git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ee3c3",
   "metadata": {},
   "source": [
    "while the second one is preparing our deployment to be stored in git and run in a docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect init --recipe docker-git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224026c",
   "metadata": {},
   "source": [
    "Assuming that we have already uploaded our flow's directory to a Github Repo, we will proceed with the second command as we want to git clone the deployment we containerized before. For this tutorial, we will start with a new deployment called \"etl-deployment-3\" but the steps can be followed directly on the \"etl-deployment-2\" too by changing its yaml configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd111c",
   "metadata": {},
   "source": [
    "The following yaml file is given. It can be observed that it is similar to the one given for the docker recipe with the difference that the pull property has some new values. These values configure the git repo we want to store our deployment's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113112e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_3\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "- prefect_docker.deployments.steps.build_docker_image:\n",
    "  id: build_image\n",
    "  requires: prefect-docker>=0.3.1\n",
    "  image_name: etl\n",
    "  tag: latest\n",
    "  dockerfile: Dockerfile\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "- prefect_docker.deployments.steps.push_docker_image: \n",
    "  requires: prefect-docker>=0.3.1\n",
    "  image_name: '{{ build_image.image_name }}'\n",
    "  tag: '{{ build_image.tag }}'\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.git_clone:\n",
    "  repository: '{{ repository }}'\n",
    "  branch: '{{ branch }}'\n",
    "  access_token: null\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name: null\n",
    "  version: null\n",
    "  tags: []\n",
    "  description: null\n",
    "  schedule: {}\n",
    "  flow_name: null\n",
    "  entrypoint: null\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: null\n",
    "    work_queue_name: null\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc13720",
   "metadata": {},
   "source": [
    "We add the tag credentials at the push property, to use the Docker Hub block as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2600cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "push:\n",
    "- prefect_docker.deployments.steps.push_docker_image:\n",
    "  requires: prefect-docker>=0.3.1\n",
    "  image_name: '{{ build-image.image_name }}'\n",
    "  tag: '{{ build-image.tag }}'\n",
    "  credentials: '{{ prefect_docker.docker-registry-credentials.docker_registry_creds_name\n",
    "    }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950aad7",
   "metadata": {},
   "source": [
    "We then create a Github Block using the Prefect's UI for the pull property. There, we set the HTTPS or SSH url for Block to be able to connect to the Repository. In our case we will use https. Then, we paste the provided code snippet inside our flow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc409b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.filesystems import GitHub\n",
    "\n",
    "github_block = GitHub.load(\"git-etl-deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639dacb8",
   "metadata": {},
   "source": [
    "Finally, we add the tag credentials at the pull property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull:\n",
    "- prefect.deployments.steps.git_clone:\n",
    "    repository: '{{ repository }}'\n",
    "    credentials: \"{{ prefect.blocks.github-credentials.git-etl-deployment }}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13320f66",
   "metadata": {},
   "source": [
    "Our final yaml file should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_3\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "- prefect_docker.deployments.steps.build_docker_image:\n",
    "    id: build_image\n",
    "    requires: prefect-docker>=0.3.1\n",
    "    image_name: docker.io/stamatissideris/etl\n",
    "    tag: latest\n",
    "    dockerfile: Dockerfile\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "- prefect_docker.deployments.steps.push_docker_image: \n",
    "    requires: prefect-docker>=0.3.1\n",
    "    image_name: '{{ build_image.image_name }}'\n",
    "    tag: '{{ build_image.tag }}'\n",
    "    credentials: '{{ prefect_docker.docker-registry-credentials.docker_registry_creds_name }}'\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.git_clone:\n",
    "    repository: '{{ repository }}'\n",
    "    credentials: \"{{ prefect.blocks.github-credentials.git-etl-deployment }}\"\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name: null\n",
    "  version: null\n",
    "  tags: []\n",
    "  description: null\n",
    "  schedule: {}\n",
    "  flow_name: null\n",
    "  entrypoint: null\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: null\n",
    "    work_queue_name: null\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccbecc",
   "metadata": {},
   "source": [
    "Finally, we are ready to build the deployment with the command \"prefect deploy\". The final yaml file after the deployment and saving of the configuration is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your prefect.yaml file! You can you this file for storing and managing\n",
    "# configuration for deploying your flows. We recommend committing this file to source\n",
    "# control along with your flow code.\n",
    "\n",
    "# Generic metadata about this project\n",
    "name: flow_3\n",
    "prefect-version: 2.10.21\n",
    "\n",
    "# build section allows you to manage and build docker images\n",
    "build:\n",
    "- prefect_docker.deployments.steps.build_docker_image:\n",
    "    id: build_image\n",
    "    requires: prefect-docker>=0.3.1\n",
    "    image_name: docker.io/stamatissideris/etl\n",
    "    tag: latest\n",
    "    dockerfile: Dockerfile\n",
    "\n",
    "# push section allows you to manage if and how this project is uploaded to remote locations\n",
    "push:\n",
    "- prefect_docker.deployments.steps.push_docker_image:\n",
    "    requires: prefect-docker>=0.3.1\n",
    "    image_name: '{{ build_image.image_name }}'\n",
    "    tag: '{{ build_image.tag }}'\n",
    "    credentials: '{{ prefect_docker.docker-registry-credentials.docker_registry_creds_name\n",
    "      }}'\n",
    "\n",
    "# pull section allows you to provide instructions for cloning this project in remote locations\n",
    "pull:\n",
    "- prefect.deployments.steps.git_clone:\n",
    "    repository: '{{ repository }}'\n",
    "    credentials: \"{{ prefect.blocks.github-credentials.git-etl-deployment }}\"\n",
    "\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name:\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  schedule: {}\n",
    "  flow_name:\n",
    "  entrypoint:\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name:\n",
    "    work_queue_name:\n",
    "    job_variables:\n",
    "      image: '{{ build_image.image }}'\n",
    "- name: etl-deployment-3\n",
    "  version:\n",
    "  tags: []\n",
    "  description:\n",
    "  entrypoint: etl.py:main_flow\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: docker-etl\n",
    "    work_queue_name:\n",
    "    job_variables: {}\n",
    "  schedule:\n",
    "    cron: 0 2 * * *\n",
    "    timezone: UTC\n",
    "    day_or: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af4942",
   "metadata": {},
   "source": [
    "The deployment is also observed in the UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626d487",
   "metadata": {},
   "source": [
    "The image is pushed at the Docker Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43330ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd55515",
   "metadata": {},
   "source": [
    "When the schedule time has arrived, the work pool pulls the flow from the Github Repo and creates an Image of it based on our custom Dockerfile. Then, it runs the updated image and pushes it to the Docker Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb7409",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6f2ea",
   "metadata": {},
   "source": [
    "In essence, this tutorial has empowered you to complete deployments of Prefect workflows. From scheduling using cronjobs, parameterization, encapsulation using Docker to version controlling via GitHub, the tutorial is analytically presenting all the steps in order to reach the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6d4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
