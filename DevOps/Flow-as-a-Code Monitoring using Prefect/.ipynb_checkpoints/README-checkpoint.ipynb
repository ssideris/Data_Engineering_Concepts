{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77be66e",
   "metadata": {},
   "source": [
    "## ETL Flow Monitoring using Prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4835308e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex;\">              <img src=\"Images/author_pic.jpg\" alt=\"author profile pic\" style=\"width:8%;                      border-radius:100%; border: 1px solid black;\"/>              <div style=\"float: right; margin-left:3%\">              <p style=\" font-size: 130%; margin-top:10%; \">By Stamatis Sideris</p>              <p style=\"font-size: 100%;\">Updated as of: August 1, 2023</p>              </div>              </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"display: flex;\"> \\\n",
    "             <img src=\"Images/author_pic.jpg\" alt=\"author profile pic\" style=\"width:8%; \\\n",
    "                     border-radius:100%; border: 1px solid black;\"/> \\\n",
    "             <div style=\"float: right; margin-left:3%\"> \\\n",
    "             <p style=\" font-size: 130%; margin-top:10%; \">By Stamatis Sideris</p> \\\n",
    "             <p style=\"font-size: 100%;\">Updated as of: August 1, 2023</p> \\\n",
    "             </div> \\\n",
    "             </div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb401002",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Prerequisites](#prerequisites)\n",
    "\n",
    "[3. Creating an ETL Flow using Python](#creating-an-etl-flow-using-python)\n",
    "\n",
    "[4. Monitoring the Flow using Prefect](#monitoring-the-flow-using-prefect)\n",
    "\n",
    "[5. Monitoring a Containerized Flow using Prefect](#monitoring-a-containerized-flow-using-prefect)\n",
    "\n",
    "[6. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9742d92",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Flow monitoring is about seamlessly coordinating the execution of various tasks or steps within a workflow, enabling efficient and reliable data processing pipelines. Prefect simplifies this process by providing a clean and intuitive API for defining and executing workflows. With Prefect, you can design your workflows as code, making them version-controlled, modular, and easy to maintain.\n",
    "\n",
    "In this tutorial, we are going to use the ETL pipeline created [here](https://github.com/ssideris/Data_Management_Concepts/blob/main/DevOps/Containerized%20ETL%20Procedures%20for%20Local%20Infrastructures%20using%20Python%2C%20Docker%20%26%20Docker%20Compose/README.md#anaconda-installation) and step-by-step monitor the flow using Prefect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b1a59",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "As always, I am going to start by using a VM instance provided by Google, as proposed [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Virtual%20Machine%20Instance%20Creation%20using%20Google%20Cloud.pdf). This is a very helpful step to deal with the low specs of your computer but also, it is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a2fa0",
   "metadata": {},
   "source": [
    "### Creating an ETL Flow using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49e585",
   "metadata": {},
   "source": [
    "The ETL flow-as-code to be used has been described [here](https://github.com/ssideris/Data_Management_Concepts/blob/main/DevOps/Containerized%20ETL%20Procedures%20for%20Local%20Infrastructures%20using%20Python%2C%20Docker%20%26%20Docker%20Compose/README.md#containerized-etl-procedures-for-local-infrastructures-using-python,-docker-and-docker-compose). The flow loads a Kaggle dataset using the Kaggle API and after some simple transformations, it loads it to a Postgres DB. For the parameterization of the flow, a .env file is used alongside the dotenv python library which enables us to use environment variables inside our python script. The main python script is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import timedelta\n",
    "import opendatasets as od\n",
    "import os\n",
    "# import dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def extract_data(path: str) :\n",
    "    # read each csv in chunks of 100000 rows\n",
    "    df_iter = pd.read_csv(path, iterator=True, chunksize=100000)\n",
    "    df = next(df_iter)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_data(df) :\n",
    "    # set the datetime format and drop a table that is full of Nulls\n",
    "    df['event_time'] = pd.to_datetime(df['event_time']) \n",
    "    df = df.drop('category_code', axis=1)\n",
    "    return df\n",
    "\n",
    "def load_data(table_name, df):\n",
    "    # create an engine that connects to the postgres database\n",
    "    engine = create_engine(f'postgresql://{os.getenv(\"user\")}:{os.getenv(\"password\")}@{os.getenv(\"host\")}:{os.getenv(\"port\")}/{os.getenv(\"db\")}')\n",
    "    # append each chunk to the table - the chunk method is needed because .to_sql function cannot handle large volume of data\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "    print(\"Finished ingesting data into the postgres database\")\n",
    "\n",
    "def download_data():\n",
    "    od.download(os.getenv(\"data_url\"))\n",
    "\n",
    "def load_env():\n",
    "    load_dotenv()\n",
    "\n",
    "def log_subflow(table_name: str):\n",
    "    print(f\"Logging Subflow for: {table_name}\")\n",
    "\n",
    "def main_flow():\n",
    "    # load environment variables\n",
    "    load_env()\n",
    "    # download dataset from Kaggle\n",
    "    download_data()\n",
    "    log_subflow(os.getenv(\"table_name\"))\n",
    "    for i in os.getenv(\"csv_name\").split(','):\n",
    "        path = os.getenv(\"data_path\") + i\n",
    "        raw_data = extract_data(path)\n",
    "        data = transform_data(raw_data)\n",
    "        load_data(os.getenv(\"table_name\"), data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8063e6d",
   "metadata": {},
   "source": [
    "The .env file is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b098346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Postgres credentials\n",
    "host = \"pgdatabase\"\n",
    "port = \"5432\"\n",
    "db = \"ecommerce_data\"\n",
    "user = \"root\"\n",
    "password = \"root\"\n",
    "table_name = \"ecommerce_data_all\"\n",
    "csv_name = \"2019-Dec.csv,2019-Nov.csv,2019-Oct.csv,2020-Jan.csv,2020-Feb.csv\"\n",
    "\n",
    "# Kaggle creds\n",
    "data_url = \"https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop/download?datasetVersionNumber=6\"\n",
    "data_path = \"ecommerce-events-history-in-cosmetics-shop/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d3023",
   "metadata": {},
   "source": [
    "In order for the script to run, we need to perform some pip installations of the libraries used. As a best practise, we will create a conda environment to install the dependencies there. For anaconda installation refer [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Containerized%20ETL%20Procedures%20for%20Local%20Infrastructures%20using%20Python%2C%20Docker%20%26%20Docker%20Compose#anaconda-installation). In order to create the virtual environment, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48058519",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name ecommerce python=3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50f1c5",
   "metadata": {},
   "source": [
    "and to activate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate ecommerce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f21d72",
   "metadata": {},
   "source": [
    "The requirements.txt we will use to pip install the dependencies inside the VE is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas==2.0.3\n",
    "prefect==2.10.21\n",
    "prefect-sqlalchemy== 0.2.4\n",
    "protobuf==4.23.4\n",
    "pyarrow==12.0.1\n",
    "pandas-gbq==0.19.2\n",
    "psycopg2-binary==2.9.6\n",
    "sqlalchemy==2.0.19\n",
    "opendatasets==0.1.22\n",
    "python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817054c3",
   "metadata": {},
   "source": [
    "and the command to use inside the VE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4900a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc0437",
   "metadata": {},
   "source": [
    "As a last step, let's create a flows directory and move our python script there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir flows\n",
    "mv ./etl.py ./flows/etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cef83",
   "metadata": {},
   "source": [
    "### Monitoring the Flow using Prefect\n",
    "\n",
    "Prefect, uses decorators to enhance the functionality and ease of defining and managing workflows. Decorators are special functions in Python that modify the behavior of other functions or methods. Prefect has two primary decorators that are commonly used:\n",
    "\n",
    "@task: The @task decorator is applied to functions that represent individual tasks in a Prefect workflow. These functions can be any Python code that performs a specific action or computation. When a function is decorated with @task, Prefect recognizes it as a unit of work and can track its execution, dependencies, and inputs/outputs automatically. This allows you to compose complex data pipelines by chaining tasks together, and Prefect ensures that tasks are executed in the correct order based on their dependencies.\n",
    "\n",
    "@flow: The @flow decorator is applied to functions that represent higher-level workflows in Prefect. These functions define the entire workflow and can include multiple tasks (which may have @task decorators) and other flows. This allows you to build more complex workflows by composing smaller units of work. When a function is decorated with @flow, Prefect recognizes it as a workflow and can orchestrate the execution of tasks within the flow.\n",
    "\n",
    "We will add a flow decorator over the main_flow function as it is the one that shows the sequence in which the tasks should be executed and then we will add a task decorator over each other function that is working as a step of the flow. Each decorator can be parameterized, setting for example a name for each task, enabling log_prints and the number of retries before failure of execution. The modified etl flow is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import timedelta\n",
    "import opendatasets as od\n",
    "import os\n",
    "# import prefect\n",
    "from prefect import task, flow\n",
    "# import dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "@task(name=\"Read Data as csv\", log_prints=True, retries=3)\n",
    "def extract_data(path: str) :\n",
    "    # read each csv in chunks of 100000 rows \n",
    "    df_iter = pd.read_csv(path, iterator=True, chunksize=100000)\n",
    "    df = next(df_iter)\n",
    "\n",
    "    return df\n",
    "\n",
    "@task(name=\"Transform data\", log_prints=True, retries=3)\n",
    "def transform_data(df) :\n",
    "    # set the datetime format and drop a table that is full of Nulls\n",
    "    df['event_time'] = pd.to_datetime(df['event_time']) \n",
    "    df = df.drop('category_code', axis=1)\n",
    "    return df\n",
    "\n",
    "@task(name=\"Load data to PostgreSQL\", log_prints=True, retries=3)\n",
    "def load_data(table_name, df):\n",
    "    # create an engine that connects to the postgres database\n",
    "    engine = create_engine(f'postgresql://{os.getenv(\"user\")}:{os.getenv(\"password\")}@{os.getenv(\"host\")}:{os.getenv(\"port\")}/{os.getenv(\"db\")}')\n",
    "    # append each chunk to the table - the chunk method is needed because .to_sql function cannot handle large volume of data\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "    print(\"Finished ingesting data into the postgres database\")\n",
    "\n",
    "@task(name=\"Download data from Kaggle\", log_prints=True, retries=3)\n",
    "def download_data():\n",
    "    od.download(os.getenv(\"data_url\"))\n",
    "\n",
    "@task(name=\"Load Environment Variables\", log_prints=True, retries=3)\n",
    "def load_env():\n",
    "    load_dotenv()\n",
    "\n",
    "def log_subflow(table_name: str):\n",
    "    print(f\"Logging Subflow for: {table_name}\")\n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow():\n",
    "    # load environment variables\n",
    "    load_env()\n",
    "    # download dataset from Kaggle\n",
    "    download_data()\n",
    "    log_subflow(os.getenv(\"table_name\"))\n",
    "    for i in os.getenv(\"csv_name\").split(','):\n",
    "        path = os.getenv(\"data_path\") + i\n",
    "        raw_data = extract_data(path)\n",
    "        data = transform_data(raw_data)\n",
    "        #load_data(os.getenv(\"table_name\"), data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb7975",
   "metadata": {},
   "source": [
    "For now, we will comment out the last task of loading the data to the postgres (#load_data(os.getenv(\"table_name\"), data)), as we have not established a connection to a postgres db yet for our flow to be able to load data there. We move on running our flow and visualizing it locally using the Prefect API and UI. To do so, first start the prefect server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99315a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect server start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148c6aa",
   "metadata": {},
   "source": [
    "then, run the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./flows/etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f2e7e",
   "metadata": {},
   "source": [
    "and visit the provided for the UI link (The UI is running by default in port 4200):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3978d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://127.0.0.1:4200/api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169a948",
   "metadata": {},
   "source": [
    "The flow should appear in the UI, in the \"Flow Runs\" tab alongside information and logging about the flow and each of their tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6ac25",
   "metadata": {},
   "source": [
    "### Monitoring a Containerized Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3dc2f",
   "metadata": {},
   "source": [
    "Next, let's containerize our Flow using Docker and Docker Compose. If you are not familiar with Docker and Docker Compose refer [here](https://github.com/ssideris/Data_Management_Concepts/tree/main/DevOps/Deployment%20of%20Local%20Containerized%20Relational%20Database%20using%20PostgreSQL%20%26%20Docker) and [here](https://github.com/ssideris/Data_Management_Concepts/blob/main/DevOps/Containerized%20ETL%20Procedures%20for%20Local%20Infrastructures%20using%20Python%2C%20Docker%20%26%20Docker%20Compose/README.md#containerized-etl-procedures-for-local-infrastructures-using-python,-docker-and-docker-compose). The scope of the containerization is to create a network of containers that will communicate with each other. The network will host four containers. The first will pull a postgres image to use as a db for our flow data and Prefect's metadata. The second will pull an image of Pgadmin which will offer us a UI for the Postgres DB. The third will pull an image that will start the local Prefect server responsible for the Prefect API and UI. The final container will host a self-made image, using a Dockerfile, that will establish the environment and run our flow. \n",
    "\n",
    "To start, we firstly create the Dockerfile that will work as the blueprint for our Flow's image. The Dockerfile is pulling a Prefect image from the DockerHub that will help us connect our flow to the Prefect server and perform some deployments later on. Next, the Dockerfile is copying the local flows directory and the kaggle.json file inside the container. Pay attention to copy the kaggle.json file in the root directory so for our script to be able to read it automatically without asking us to pass the kaggle credentials. Moreover, a pip installation of all the needed libraries inside our python script is happening using the requirements.txt. Finally the command line is responsible to run our flow inside the etl.py when the image is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc26148",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM prefecthq/prefect:2.10.17-python3.11\n",
    "\n",
    "ADD flows /opt/prefect/flows\n",
    "\n",
    "COPY kaggle.json /opt/prefect/kaggle.json \n",
    "\n",
    "RUN pip install -r /opt/prefect/flows/requirements.txt\n",
    "\n",
    "CMD [\"python\", \"/opt/prefect/flows/etl.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f342ae9",
   "metadata": {},
   "source": [
    "We then create a docker-compose.yml file to create the network. Four services are created:\n",
    "\n",
    "The pgdatabase is pulling a postgres 13 image to use as Database for our flow. The credentials for the server are provided as environment variables. A local data directory is mounted to the metadata of the postgres inside the container for keeping consistency. Finally, the container's port 5432 is mounted to the local port 5432 in order to access the db from outside the container.\n",
    "\n",
    "The pgadmin is also pulling an image of the pgadmin to provide as with a UI for our postgres DB, together with some access credentials and a mounting of port 8080 to local port 80. \n",
    "\n",
    "The server is pulling a prefect image and starts the Prefect server using a shell script provided by the image. The environment variables provided are helping us to mount the UI and API to a url so we can access them from the browser. Moreover the variable PREFECT_SERVER_API_HOST is helping us to configure the correct host of the Docker container as it differs from the one we use locally. The variable PREFECT_API_DATABASE_CONNECTION_URL is configuring a connection to the postgres db hosted in the pgdatabase service in order to store prefect's metadata.\n",
    "\n",
    "Finally, the etl_flow builds our image using the Dockerfile created before and runs our flow. It is important to set the environment variable PREFECT_API_URL in order for our flow to run on the prefect server started by the server service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "services:\n",
    "  # Postgres DB\n",
    "  pgdatabase:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      - POSTGRES_USER=root\n",
    "      - POSTGRES_PASSWORD=root\n",
    "      - POSTGRES_HOST=pgdatabase\n",
    "      - POSTGRES_DB=ecommerce_data\n",
    "    volumes:\n",
    "      - db:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "\n",
    "  # PGAdmin UI for Postgres\n",
    "  pgadmin:\n",
    "    image: dpage/pgadmin4:latest\n",
    "    environment:\n",
    "      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n",
    "      - PGADMIN_DEFAULT_PASSWORD=root\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "\n",
    "  # Prefect Server API and UI\n",
    "  server:\n",
    "    image: prefecthq/prefect:2.10.17-python3.11\n",
    "    volumes:\n",
    "      - prefect:/root/.prefect\n",
    "    entrypoint:\n",
    "      [\n",
    "        \"/opt/prefect/entrypoint.sh\",\n",
    "        \"prefect\",\n",
    "        \"server\",\n",
    "        \"start\"\n",
    "      ]\n",
    "    environment:\n",
    "      - PREFECT_UI_URL=http://127.0.0.1:4200/api\n",
    "      - PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "      - PREFECT_SERVER_API_HOST=0.0.0.0\n",
    "      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://root:root@pgdatabase:5432/ecommerce_data\n",
    "    ports:\n",
    "      - 4200:4200\n",
    "    depends_on:\n",
    "      - pgdatabase\n",
    "\n",
    "  # Flow\n",
    "  etl_flow:\n",
    "    build: .\n",
    "    image: ecommerce_etl\n",
    "    working_dir: \"/root/flows\"\n",
    "    volumes:\n",
    "      - \"./flows:/root/flows\"\n",
    "    environment:\n",
    "      - PREFECT_API_URL=http://server:4200/api\n",
    "    depends_on:\n",
    "      - server\n",
    "\n",
    "volumes:\n",
    "  prefect:\n",
    "  db:\n",
    "networks:\n",
    "  default:\n",
    "    name: prefect-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573914e",
   "metadata": {},
   "source": [
    "We use the following command to run the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21f312",
   "metadata": {},
   "source": [
    "The containers should start running alongside our etl flow. The loaded data can be accessed using the pgadmin UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbe41de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[image.png]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![image.png](Images/Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fa2fe",
   "metadata": {},
   "source": [
    "The Flow can be examined via Prefect using the following link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://0.0.0.0:4200/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image.png](Images/Picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73033cb7",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9503b4",
   "metadata": {},
   "source": [
    "Overall, Prefect is a great tool for developing a flow-as-a-code and monitoring it in a simple way. From development to production, Prefect could offer its users a clear view of the steps executed inside a Flow and any points of failure. This could significantly decrease the amount of time and cost needed to develop a pipeline, making the development more independent over other software tools and more explainable to other users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
